{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 412kB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 479kB/s]\n",
      "README.md: 100%|██████████| 10.7k/10.7k [00:00<00:00, 21.5MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 314kB/s]\n",
      "config.json: 100%|██████████| 612/612 [00:00<00:00, 1.87MB/s]\n",
      "pytorch_model.bin:  46%|████▌     | 41.9M/90.9M [02:03<02:08, 382kB/s]Error while downloading from https://cdn-lfs.huggingface.co/sentence-transformers/all-MiniLM-L6-v2/c3a85f238711653950f6a79ece63eb0ea93d76f6a6284be04019c53733baf256?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1709108190&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTEwODE5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9zZW50ZW5jZS10cmFuc2Zvcm1lcnMvYWxsLU1pbmlMTS1MNi12Mi9jM2E4NWYyMzg3MTE2NTM5NTBmNmE3OWVjZTYzZWIwZWE5M2Q3NmY2YTYyODRiZTA0MDE5YzUzNzMzYmFmMjU2P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=t2YZH4GFzwtoXc2D36zTfNkzEBCfadqqmTnJ6dqDpR-ZEUjjAyYF6cyqd0FkvXEmNTs8ME4IcwoGyVxHL2qcHlX1vnEddLdtFPJlOieN5u-Jo5Mw7gP8u-2hIAH3DG7-oRaE4HchEqRFaDCJyrX08S17C9UBJk0-rCnoknXcXVsGeKzUSG5oRd5LSKUjnx2DFTM8%7EuT9O%7EOf0PRZy2Vh5J6VUaOxFkgdeIYlYXGmcW1p-zgFhSpb5ul9TWPiP8VSCFF7289mncYH8WG%7EH8QHp5-SJLykshWxAXyZziOFN3H4Ogu6T2yAW5aLgxgQHbUy4nWJH%7EnuOkRbzmqi7IOebw__&Key-Pair-Id=KVTP0A1DKRTAX: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "pytorch_model.bin: 100%|██████████| 90.9M/90.9M [01:49<00:00, 447kB/s]\n",
      "pytorch_model.bin:  46%|████▌     | 41.9M/90.9M [04:15<04:57, 164kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<00:00, 400kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 566kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 688kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 949kB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 706kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created and saved to /Users/dadidelux/Desktop/alfio_dev/pkl_output/mabuhay_price.pkl\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open titles_index.pkl for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gm/984sqkyx61s5_ryl4nl589440000gn/T/ipykernel_17091/2305096066.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mcreate_faiss_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Example title\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0msimilar_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_similar_titles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"titles_index.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Similar titles for '{query}':\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Index: {i}, Distance: {dist}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/gm/984sqkyx61s5_ryl4nl589440000gn/T/ipykernel_17091/2305096066.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(query_title, faiss_pkl, top_k)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msearch_similar_titles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaiss_pkl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Load the FAISS index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaiss_pkl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Load the Sentence Transformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all-MiniLM-L6-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/alfio_dev/.alfio_env/lib/python3.10/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   9923\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9924\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/faiss/faiss/impl/io.cpp:68: Error: 'f' failed: could not open titles_index.pkl for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# Define the path to the alfio_dev folder\n",
    "alfio_dev_path = \"/Users/dadidelux/Desktop/alfio_dev/\"\n",
    "\n",
    "# Construct the path to the CSV file\n",
    "csv_file_path = os.path.join(alfio_dev_path, \"data\", \"mabuhay_price.csv\")\n",
    "output_file_path = os.path.join(alfio_dev_path, \"pkl_output\", \"mabuhay_price.pkl\")\n",
    "\n",
    "\n",
    "def create_faiss_index(input_csv, output_pkl):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Extract titles from the 'mergedata' column\n",
    "    titles = df[\"mergedata\"].tolist()\n",
    "\n",
    "    # Load a pre-trained Sentence Transformer model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Encode the titles to get sentence embeddings\n",
    "    embeddings = model.encode(titles, convert_to_tensor=True)\n",
    "\n",
    "    # Convert embeddings to numpy array\n",
    "    embeddings_np = embeddings.cpu().detach().numpy()\n",
    "\n",
    "    # Create a FAISS index\n",
    "    dimension = embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_np)\n",
    "\n",
    "    # Save the FAISS index to a pickle file\n",
    "    faiss.write_index(index, output_pkl)\n",
    "\n",
    "    print(f\"FAISS index created and saved to {output_pkl}\")\n",
    "\n",
    "\n",
    "def search_similar_titles(query_title, faiss_pkl, top_k=5):\n",
    "    # Load the FAISS index\n",
    "    index = faiss.read_index(faiss_pkl)\n",
    "\n",
    "    # Load the Sentence Transformer model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Encode the query title\n",
    "    query_embedding = model.encode([query_title], convert_to_tensor=True)\n",
    "    query_embedding_np = query_embedding.cpu().detach().numpy()\n",
    "\n",
    "    # Search for similar titles\n",
    "    distances, indices = index.search(query_embedding_np, top_k)\n",
    "\n",
    "    return indices[0], distances[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    create_faiss_index(csv_file_path, output_file_path)\n",
    "\n",
    "    query = \"Example title\"\n",
    "    similar_indices, distances = search_similar_titles(query, \"titles_index.pkl\")\n",
    "    print(f\"Similar titles for '{query}':\")\n",
    "    for i, dist in zip(similar_indices, distances):\n",
    "        print(f\"Index: {i}, Distance: {dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing FAISS index.\n",
      "Similar titles for 'Example title':\n",
      "Index: 351, Distance: 1.605818748474121\n",
      "Index: 314, Distance: 1.6727250814437866\n",
      "Index: 270, Distance: 1.674799919128418\n",
      "Index: 257, Distance: 1.6962765455245972\n",
      "Index: 386, Distance: 1.697189211845398\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# Define the path to the alfio_dev folder\n",
    "alfio_dev_path = \"/Users/dadidelux/Desktop/alfio_dev/\"\n",
    "\n",
    "# Construct the path to the CSV file and the output PKL file\n",
    "csv_file_path = os.path.join(alfio_dev_path, \"data\", \"mabuhay_price.csv\")\n",
    "output_file_path = os.path.join(alfio_dev_path, \"pkl_output\", \"mabuhay_price.pkl\")\n",
    "\n",
    "\n",
    "def create_faiss_index(input_csv, output_pkl):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Extract titles from the 'mergedata' column\n",
    "    titles = df[\"mergedata\"].tolist()\n",
    "\n",
    "    # Load a pre-trained Sentence Transformer model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Encode the titles to get sentence embeddings\n",
    "    embeddings = model.encode(titles, convert_to_tensor=True)\n",
    "\n",
    "    # Convert embeddings to numpy array\n",
    "    embeddings_np = embeddings.cpu().detach().numpy()\n",
    "\n",
    "    # Create a FAISS index\n",
    "    dimension = embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_np)\n",
    "\n",
    "    # Save the FAISS index to a pickle file\n",
    "    faiss.write_index(index, output_pkl)\n",
    "\n",
    "    print(f\"FAISS index created and saved to {output_pkl}\")\n",
    "\n",
    "\n",
    "def search_similar_titles(query_title, faiss_pkl, top_k=5):\n",
    "    # Load the FAISS index\n",
    "    index = faiss.read_index(faiss_pkl)\n",
    "\n",
    "    # Load the Sentence Transformer model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Encode the query title\n",
    "    query_embedding = model.encode([query_title], convert_to_tensor=True)\n",
    "    query_embedding_np = query_embedding.cpu().detach().numpy()\n",
    "\n",
    "    # Search for similar titles\n",
    "    distances, indices = index.search(query_embedding_np, top_k)\n",
    "\n",
    "    return indices[0], distances[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if the PKL file exists\n",
    "    if not os.path.exists(output_file_path):\n",
    "        print(\"Creating FAISS index...\")\n",
    "        create_faiss_index(csv_file_path, output_file_path)\n",
    "    else:\n",
    "        print(\"Using existing FAISS index.\")\n",
    "\n",
    "    # Example usage\n",
    "    query = \"Example title\"\n",
    "    similar_indices, distances = search_similar_titles(query, output_file_path)\n",
    "    print(f\"Similar titles for '{query}':\")\n",
    "    for i, dist in zip(similar_indices, distances):\n",
    "        print(f\"Index: {i}, Distance: {dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing FAISS index.\n",
      "Similar titles for 'Frozen Food Medium Box 15kg 2000':\n",
      "Title: Frozen Food Medium Box 15kg 2000, Shipping Price: 120, Distance: 1.0\n",
      "Title: Frozen Food Medium Box 9kg 1500, Shipping Price: 100, Distance: 0.9005154967308044\n",
      "Title: Frozen Food small box 5kg 2000, Shipping Price: 150, Distance: 0.8528404831886292\n",
      "Title: FROZEN PRODUCTS medium box 8kg 1500, Shipping Price: 80, Distance: 0.7883222699165344\n",
      "Title: FROZEN PRODUCTS Medium Box 20kg 1500, Shipping Price: 250, Distance: 0.7873414754867554\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# Define the path to the alfio_dev folder\n",
    "alfio_dev_path = \"/Users/dadidelux/Desktop/alfio_dev/\"\n",
    "\n",
    "# Construct the path to the CSV file and the output PKL file\n",
    "csv_file_path = os.path.join(alfio_dev_path, \"data\", \"mabuhay_price.csv\")\n",
    "output_file_path = os.path.join(alfio_dev_path, \"pkl_output\", \"mabuhay_price.pkl\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "\n",
    "def create_faiss_index(dataframe, output_pkl):\n",
    "    # Extract titles from the 'mergedata' column\n",
    "    titles = dataframe[\"mergedata\"].tolist()\n",
    "\n",
    "    # Load a pre-trained Sentence Transformer model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Encode the titles to get sentence embeddings\n",
    "    embeddings = model.encode(titles, convert_to_tensor=True)\n",
    "\n",
    "    # Convert embeddings to numpy array\n",
    "    embeddings_np = embeddings.cpu().detach().numpy()\n",
    "\n",
    "    # Create a FAISS index\n",
    "    dimension = embeddings_np.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings_np)\n",
    "\n",
    "    # Save the FAISS index to a pickle file\n",
    "    faiss.write_index(index, output_pkl)\n",
    "\n",
    "    print(f\"FAISS index created and saved to {output_pkl}\")\n",
    "\n",
    "\n",
    "def search_similar_titles(query_title, dataframe, faiss_pkl, top_k=5):\n",
    "    # Load the FAISS index\n",
    "    index = faiss.read_index(faiss_pkl)\n",
    "\n",
    "    # Load the Sentence Transformer model\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Encode the query title\n",
    "    query_embedding = model.encode([query_title], convert_to_tensor=True)\n",
    "    query_embedding_np = query_embedding.cpu().detach().numpy()\n",
    "\n",
    "    # Search for similar titles\n",
    "    distances, indices = index.search(query_embedding_np, top_k)\n",
    "\n",
    "    # Convert distances to similarity scores (range 1.0 to 0.0)\n",
    "    similarities = 1 - distances[0]\n",
    "\n",
    "    # Get the similar titles and their shipping prices\n",
    "    similar_titles = dataframe.iloc[indices[0]][\"mergedata\"]\n",
    "    shipping_prices = dataframe.iloc[indices[0]][\"shippingfee\"]\n",
    "\n",
    "    return similar_titles, shipping_prices, similarities\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if the PKL file exists\n",
    "    if not os.path.exists(output_file_path):\n",
    "        print(\"Creating FAISS index...\")\n",
    "        create_faiss_index(df, output_file_path)\n",
    "    else:\n",
    "        print(\"Using existing FAISS index.\")\n",
    "\n",
    "    # Example usage\n",
    "    query = input(\"Please input the type of shipping\")\n",
    "    similar_titles, shipping_prices, distances = search_similar_titles(\n",
    "        query, df, output_file_path\n",
    "    )\n",
    "    print(f\"Similar titles for '{query}':\")\n",
    "    for title, price, dist in zip(similar_titles, shipping_prices, distances):\n",
    "        print(f\"Title: {title}, Shipping Price: {price}, Distance: {dist}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".alfio_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
